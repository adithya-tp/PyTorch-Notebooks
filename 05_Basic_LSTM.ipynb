{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-Basic-LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithya-tp/PyTorch-Notebooks/blob/master/05_Basic_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWyCut2h6e2e",
        "colab_type": "text"
      },
      "source": [
        "# ***Imports, and initializing an LSTM Layer for a manual forward pass***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6bNFrYzx5NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60_qPtuG169c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 5\n",
        "\n",
        "# the cell-state/long-term-memory, as well as the hidden-state/short-term-memory\n",
        "# will have the same dimensions.\n",
        "hidden_dim = 10\n",
        "\n",
        "# the number of lstm layers stacked on top of each other.\n",
        "n_layers = 1\n",
        "\n",
        "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ZkOrKM6xz2",
        "colab_type": "text"
      },
      "source": [
        "# ***Setting up random input data, and seed hidden states***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2c5BDPr2w3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since the input_dim = 5 we need an input tensor of dimension (1,1,5) ==> (batch_size, sequence_length, input_dimension)\n",
        "\n",
        "# let us now initialize the seed hidden state and cell state\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "hidden = (hidden_state, cell_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wspvtGGl64su",
        "colab_type": "text"
      },
      "source": [
        "# ***Trying out forward passes by varying input dimensions***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5oGOgX14CXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4ee296f5-9c70-461c-b08c-fa92558bbaaf"
      },
      "source": [
        "# let us now feed in the input state and the hidden state tuple and examine the shapes of the returned entities\n",
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(out.shape)\n",
        "print(hidden)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 10])\n",
            "(tensor([[[ 0.0979, -0.0686,  0.3446,  0.2694,  0.1738,  0.0470,  0.2571,\n",
            "           0.0373,  0.0210, -0.4024]]], grad_fn=<StackBackward>), tensor([[[ 0.2328, -0.1534,  0.6161,  0.4863,  0.3863,  0.1028,  0.5925,\n",
            "           0.0698,  0.0467, -0.5926]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r7W0W2g46hG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0111279e-d36a-4f7d-f3fe-efb6ca8c7bc0"
      },
      "source": [
        "# let us now increase the length of our sequence and see how this changes things\n",
        "seq_len = 3\n",
        "inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(out.shape)\n",
        "print(hidden)\n",
        "\n",
        "\"\"\"\n",
        "The second dimension of \"out\" has changed, and now corresponds to the length of the input sequence we fed into the network.\n",
        "We can just feed in all the outputs in this dimension (contained in the variable out) to a fully connected layer, and\n",
        "therefore get out the predictions at all previous timesteps. This could be useful in text-generation tasks (many-to-many)\n",
        "\n",
        "For many-to-one tasks, like sentiment analysis, you can just feed in the contents of \"hidden\" into an fc layer.\n",
        "\"\"\"\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 10])\n",
            "(tensor([[[ 0.0101,  0.0985,  0.1424, -0.0549,  0.2571, -0.0055, -0.0843,\n",
            "          -0.1322,  0.0530,  0.0386]]], grad_fn=<StackBackward>), tensor([[[ 0.0352,  0.2537,  0.2543, -0.1307,  0.7461, -0.0097, -0.1506,\n",
            "          -0.1896,  0.1252,  0.0726]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}