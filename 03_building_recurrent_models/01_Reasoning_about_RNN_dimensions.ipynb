{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our vocab.\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking up a single slice of bread and feeding it in to the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(50)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "# hidden vector dims [num_layers * num_directions, batch_size, hidden_size]\n",
    "hidden = Variable(torch.randn(1, 1, 2))\n",
    "\n",
    "\"\"\"\n",
    "Here's something to remember what the input size is.\n",
    "What is the smallest unit / token in your (dataset) / (task you're trying to solve) ?\n",
    "Well here, it's a letter.\n",
    "Well then, the length of the numerical representation of that unit is the input size.\n",
    "\"\"\"\n",
    "# input dimensions [batch_size, seq_len, input_size]\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.5393,  0.9402]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[-0.5393,  0.9402]]], grad_fn=<StackBackward>)\n",
      "one input size: torch.Size([1, 1, 4]), out size: torch.Size([1, 1, 2])\n",
      "\n",
      "out: tensor([[[0.0814, 0.3760]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[0.0814, 0.3760]]], grad_fn=<StackBackward>)\n",
      "one input size: torch.Size([1, 1, 4]), out size: torch.Size([1, 1, 2])\n",
      "\n",
      "out: tensor([[[0.2418, 0.5283]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[0.2418, 0.5283]]], grad_fn=<StackBackward>)\n",
      "one input size: torch.Size([1, 1, 4]), out size: torch.Size([1, 1, 2])\n",
      "\n",
      "out: tensor([[[0.3517, 0.4476]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[0.3517, 0.4476]]], grad_fn=<StackBackward>)\n",
      "one input size: torch.Size([1, 1, 4]), out size: torch.Size([1, 1, 2])\n",
      "\n",
      "out: tensor([[[-0.1629,  0.4402]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[-0.1629,  0.4402]]], grad_fn=<StackBackward>)\n",
      "one input size: torch.Size([1, 1, 4]), out size: torch.Size([1, 1, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we are slicing up a single slice of bread row by row and feeding that to the LSTM.\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"out: {}\".format(out))\n",
    "    print(\"hidden: {}\".format(hidden))\n",
    "    print(\"one input size: {}, out size: {}\".format(one.size(), out.size()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do better: Let's feed the whole slice of bread in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "hidden = Variable(torch.randn(1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.5393,  0.9402],\n",
      "         [ 0.0814,  0.3760],\n",
      "         [ 0.2418,  0.5283],\n",
      "         [ 0.3517,  0.4476],\n",
      "         [-0.1629,  0.4402]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[-0.1629,  0.4402]]], grad_fn=<StackBackward>)\n",
      "sequence input size torch.Size([1, 5, 4]) out size torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# However, we can send in this single slice of bread at once into the rnn,\n",
    "# and get the corresponding output tensor for the entire slice.\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"out: {}\".format(out))\n",
    "print(\"hidden: {}\".format(hidden))\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heck, lets feed in an entire loaf of bread dawgg!\n",
    "[Just make sure your RNN has made arrangements to accomodate the same.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.5393,  0.9402],\n",
      "         [ 0.0814,  0.3760],\n",
      "         [ 0.2418,  0.5283],\n",
      "         [ 0.3517,  0.4476],\n",
      "         [-0.1629,  0.4402]],\n",
      "\n",
      "        [[-0.8194,  0.4513],\n",
      "         [-0.3054,  0.7992],\n",
      "         [ 0.4408,  0.6273],\n",
      "         [ 0.4288,  0.3472],\n",
      "         [ 0.2667,  0.3854]],\n",
      "\n",
      "        [[-0.0508,  0.5430],\n",
      "         [ 0.3264,  0.5620],\n",
      "         [-0.0527, -0.0152],\n",
      "         [-0.4468,  0.2586],\n",
      "         [ 0.1002,  0.7090]]], grad_fn=<TransposeBackward1>)\n",
      "hidden: tensor([[[-0.1629,  0.4402],\n",
      "         [ 0.2667,  0.3854],\n",
      "         [ 0.1002,  0.7090]]], grad_fn=<StackBackward>)\n",
      "sequence input size torch.Size([3, 5, 4]) out size torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# \"Thats not good enough... We have to go deeper...\"\n",
    "\"\"\"\n",
    "We fed in the single slice of bread to get the corresponding \"hidden\" version of the bread slice.\n",
    "Now, we can feed in multiple bread slices to make things go even faster!! Isn't that cool?\n",
    "We must initialize our hidden state such that it can accept multiple slices of data.\n",
    "\"\"\"\n",
    "\n",
    "torch.manual_seed(50)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"out: {}\".format(out))\n",
    "print(\"hidden: {}\".format(hidden))\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we didn't specify batch=True when we created our inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.5393,  0.9402],\n",
      "         [-0.8194,  0.4513],\n",
      "         [-0.0508,  0.5430]],\n",
      "\n",
      "        [[ 0.0814,  0.3760],\n",
      "         [-0.3054,  0.7992],\n",
      "         [ 0.3264,  0.5620]],\n",
      "\n",
      "        [[ 0.2418,  0.5283],\n",
      "         [ 0.4408,  0.6273],\n",
      "         [-0.0527, -0.0152]],\n",
      "\n",
      "        [[ 0.3517,  0.4476],\n",
      "         [ 0.4288,  0.3472],\n",
      "         [-0.4468,  0.2586]],\n",
      "\n",
      "        [[-0.1629,  0.4402],\n",
      "         [ 0.2667,  0.3854],\n",
      "         [ 0.1002,  0.7090]]], grad_fn=<StackBackward>)\n",
      "hidden: tensor([[[-0.1629,  0.4402],\n",
      "         [ 0.2667,  0.3854],\n",
      "         [ 0.1002,  0.7090]]], grad_fn=<StackBackward>)\n",
      "sequence input size torch.Size([5, 3, 4]) out size torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# What if we didn't specify the batch first parameter (because the default value for this is false)\n",
    "# in pytorch. Now if the RNN we built must accept this input data, we must first swap the first two\n",
    "# dimensions. This results in a dimension change from (seq_len, batch_size, input_size) to\n",
    "# (batch_size, seq_len, input_size), that is, to what it was when batch_first was set to true.\n",
    "torch.manual_seed(50)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2)\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "inputs = inputs.transpose(dim0=0, dim1=1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"out: {}\".format(out))\n",
    "print(\"hidden: {}\".format(hidden))\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
