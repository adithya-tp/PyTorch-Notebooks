{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "    1. This is just the abstraction you need to feed in a big dataset in batches, almost like an iterator.\n",
    "    2. You can override the base class magic functions of __len__ and __getitem__ for your custom dataset.\n",
    "\n",
    "    A couple technical terms worth knowing:\n",
    "        1. Pass -> One forward pass of batch + One backward pass of batch\n",
    "        2. Batch Size -> The number of training examples in one forward/backward pass.\n",
    "        3. Iterations -> The number of passes required to process all data points in the dataset (say, training data).\n",
    "        4. Epoch -> One pass over all data points. One or more iterations make up an epoch.\n",
    "        \n",
    "        [So a dataset with 1000 examples and a batch size of 500, would need 2 iterations to complete an epoch]\n",
    "        \n",
    "    One could code up an implementation to randomly shuffle and slice up data, but it could turn out to be cumbersome\n",
    "    and takes away your time to spend on the more important stuff. Data Loader comes to our rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader Class for the diabetes classification dataset.\n",
    "\n",
    "    This dataset contains 8 features and the target column contains 0 \n",
    "    or 1 indicating whether the ith person has diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must inherit from the Dataset class of pytorch\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # loadtxt assumes that each row has the same number of values.\n",
    "        # (if you'd like to handle missing values, use genfromtxt function instead)\n",
    "        data = np.loadtxt('data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        self.x_data = torch.from_numpy(data[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(data[:, -1])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert (0 <= index < self.len), \"That doesn't look like a valid index dudette/dude\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an object from the DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset=DiabetesDataset(),\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using train_data_loader as a data supplier in the Diabetes Classfication Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the class for the nn Classifier (Functional paradigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiabetesNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=8, out_features=6)\n",
    "        self.fc2 = nn.Linear(in_features=6, out_features=4)\n",
    "        self.out = nn.Linear(in_features=4, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network object creations, defining loss and optimizer\n",
    "net = DiabetesNet()\n",
    "loss_criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Iteration: 1\n",
      "Loss: 0.6892212629318237\n",
      "Epoch : 1\n",
      "Iteration: 2\n",
      "Loss: 0.664035975933075\n",
      "Epoch : 1\n",
      "Iteration: 3\n",
      "Loss: 0.6139621138572693\n",
      "Epoch : 1\n",
      "Iteration: 4\n",
      "Loss: 0.7150481939315796\n",
      "Epoch : 1\n",
      "Iteration: 5\n",
      "Loss: 0.5498839616775513\n",
      "Epoch : 1\n",
      "Iteration: 6\n",
      "Loss: 0.556950032711029\n",
      "Epoch : 1\n",
      "Iteration: 7\n",
      "Loss: 0.4442341923713684\n",
      "Epoch : 1\n",
      "Iteration: 8\n",
      "Loss: 0.4616973400115967\n",
      "Epoch : 1\n",
      "Iteration: 9\n",
      "Loss: 0.3781052529811859\n",
      "Epoch : 1\n",
      "Iteration: 10\n",
      "Loss: 0.6124705076217651\n",
      "Epoch : 1\n",
      "Iteration: 11\n",
      "Loss: 0.461191326379776\n",
      "Epoch : 1\n",
      "Iteration: 12\n",
      "Loss: 0.5773187279701233\n",
      "Epoch : 1\n",
      "Iteration: 13\n",
      "Loss: 0.5445713400840759\n",
      "Epoch : 1\n",
      "Iteration: 14\n",
      "Loss: 0.5822882652282715\n",
      "Epoch : 1\n",
      "Iteration: 15\n",
      "Loss: 0.5311828255653381\n",
      "Epoch : 1\n",
      "Iteration: 16\n",
      "Loss: 0.9326416850090027\n",
      "Epoch : 1\n",
      "Iteration: 17\n",
      "Loss: 0.3803958296775818\n",
      "Epoch : 1\n",
      "Iteration: 18\n",
      "Loss: 0.3509477972984314\n",
      "Epoch : 1\n",
      "Iteration: 19\n",
      "Loss: 0.37157270312309265\n",
      "Epoch : 1\n",
      "Iteration: 20\n",
      "Loss: 0.6579140424728394\n",
      "Epoch : 1\n",
      "Iteration: 21\n",
      "Loss: 0.5113435387611389\n",
      "Epoch : 1\n",
      "Iteration: 22\n",
      "Loss: 0.48899349570274353\n",
      "Epoch : 1\n",
      "Iteration: 23\n",
      "Loss: 0.5097162127494812\n",
      "Epoch : 1\n",
      "Iteration: 24\n",
      "Loss: 0.5907987356185913\n",
      "Epoch : 2\n",
      "Iteration: 1\n",
      "Loss: 0.505099356174469\n",
      "Epoch : 2\n",
      "Iteration: 2\n",
      "Loss: 0.5256634950637817\n",
      "Epoch : 2\n",
      "Iteration: 3\n",
      "Loss: 0.5666654706001282\n",
      "Epoch : 2\n",
      "Iteration: 4\n",
      "Loss: 0.5184277296066284\n",
      "Epoch : 2\n",
      "Iteration: 5\n",
      "Loss: 0.4436623156070709\n",
      "Epoch : 2\n",
      "Iteration: 6\n",
      "Loss: 0.5023350119590759\n",
      "Epoch : 2\n",
      "Iteration: 7\n",
      "Loss: 0.4824383556842804\n",
      "Epoch : 2\n",
      "Iteration: 8\n",
      "Loss: 0.3859701156616211\n",
      "Epoch : 2\n",
      "Iteration: 9\n",
      "Loss: 0.42458605766296387\n",
      "Epoch : 2\n",
      "Iteration: 10\n",
      "Loss: 0.46603938937187195\n",
      "Epoch : 2\n",
      "Iteration: 11\n",
      "Loss: 0.38811826705932617\n",
      "Epoch : 2\n",
      "Iteration: 12\n",
      "Loss: 0.532563328742981\n",
      "Epoch : 2\n",
      "Iteration: 13\n",
      "Loss: 0.582718014717102\n",
      "Epoch : 2\n",
      "Iteration: 14\n",
      "Loss: 0.44213345646858215\n",
      "Epoch : 2\n",
      "Iteration: 15\n",
      "Loss: 0.6295725703239441\n",
      "Epoch : 2\n",
      "Iteration: 16\n",
      "Loss: 0.4619196057319641\n",
      "Epoch : 2\n",
      "Iteration: 17\n",
      "Loss: 0.47358208894729614\n",
      "Epoch : 2\n",
      "Iteration: 18\n",
      "Loss: 0.6160916686058044\n",
      "Epoch : 2\n",
      "Iteration: 19\n",
      "Loss: 0.5583627820014954\n",
      "Epoch : 2\n",
      "Iteration: 20\n",
      "Loss: 0.47206857800483704\n",
      "Epoch : 2\n",
      "Iteration: 21\n",
      "Loss: 0.42853984236717224\n",
      "Epoch : 2\n",
      "Iteration: 22\n",
      "Loss: 0.45590096712112427\n",
      "Epoch : 2\n",
      "Iteration: 23\n",
      "Loss: 0.46155479550361633\n",
      "Epoch : 2\n",
      "Iteration: 24\n",
      "Loss: 0.6170873045921326\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(2):\n",
    "    # this is where the dataloader object helps out a lot\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        # get input batch of data\n",
    "        batch_train, batch_labels = data\n",
    "        \n",
    "        # [The following paragraphs are from pytorch tutorials - personal note]\n",
    "        # A PyTorch Variable is a wrapper around a PyTorch Tensor, and represents \n",
    "        # a node in a computational graph. If x is a Variable then x.data is a \n",
    "        # Tensor giving its value, and x.grad is another Variable holding the gradient \n",
    "        # of x with respect to some scalar value.\n",
    "\n",
    "        # PyTorch Variables have the same API as PyTorch tensors: (almost) any operation \n",
    "        # you can do on a Tensor you can also do on a Variable; the difference is that \n",
    "        # autograd allows you to automatically compute gradients.\n",
    "        batch_train, batch_labels = Variable(batch_train), Variable(batch_labels)\n",
    "        \n",
    "        # getting predictions from model\n",
    "        batch_pred = net(batch_train)\n",
    "        \n",
    "        # Compute and print loss\n",
    "        loss = loss_criterion(batch_pred.squeeze(1), batch_labels)\n",
    "        print(\"Epoch : {}\\nIteration: {}\\nLoss: {}\".format(epoch + 1, i + 1, loss))\n",
    "        \n",
    "        # Zero out gradients, do backward pass and further, update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
