{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the class for Vanilla FFN\n",
    "class VanillaFFN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(VanillaFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=input_size // 2)\n",
    "        self.fc2 = nn.Linear(in_features=input_size // 2, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# model sanity_check\n",
    "sample_net = VanillaFFN(784, 10)\n",
    "print(sample_net(torch.randn(64, 784)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "train_data = datasets.MNIST(root='./data', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor()\n",
    "                                   ]),\n",
    "                                   download=True)\n",
    "# Set up the data loader for our training data.\n",
    "train_data_loader = DataLoader(dataset=train_data, \n",
    "                               shuffle=True,\n",
    "                               batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 28, 28]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# sanity check for train data loader.\n",
    "for idx, data_tuple in enumerate(train_data_loader):\n",
    "    print(data_tuple[0].shape, data_tuple[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and setup device tensor computations should be assigned to\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:, Iteration: 1, Loss: 2.317263603210449\n",
      "Epoch 1:, Iteration: 2, Loss: 94.96660614013672\n",
      "Epoch 1:, Iteration: 3, Loss: 213.5109100341797\n",
      "Epoch 1:, Iteration: 4, Loss: 122.19387817382812\n",
      "Epoch 1:, Iteration: 5, Loss: 120.39815521240234\n",
      "Epoch 1:, Iteration: 6, Loss: 94.13008117675781\n",
      "Epoch 1:, Iteration: 7, Loss: 74.5239486694336\n",
      "Epoch 1:, Iteration: 8, Loss: 32.79188919067383\n",
      "Epoch 1:, Iteration: 9, Loss: 15.758865356445312\n",
      "Epoch 1:, Iteration: 10, Loss: 4.098278999328613\n",
      "Epoch 1:, Iteration: 11, Loss: 2.6491663455963135\n",
      "Epoch 1:, Iteration: 12, Loss: 4.110940933227539\n",
      "Epoch 1:, Iteration: 13, Loss: 3.3856558799743652\n",
      "Epoch 1:, Iteration: 14, Loss: 2.589475631713867\n",
      "Epoch 1:, Iteration: 15, Loss: 2.0008091926574707\n",
      "Epoch 1:, Iteration: 16, Loss: 1.8412837982177734\n",
      "Epoch 1:, Iteration: 17, Loss: 1.5358116626739502\n",
      "Epoch 1:, Iteration: 18, Loss: 1.553002119064331\n",
      "Epoch 1:, Iteration: 19, Loss: 1.5871416330337524\n",
      "Epoch 1:, Iteration: 20, Loss: 1.66726553440094\n",
      "Epoch 1:, Iteration: 21, Loss: 1.6304442882537842\n",
      "Epoch 1:, Iteration: 22, Loss: 1.405110239982605\n",
      "Epoch 1:, Iteration: 23, Loss: 1.870603084564209\n",
      "Epoch 1:, Iteration: 24, Loss: 1.4011108875274658\n",
      "Epoch 1:, Iteration: 25, Loss: 1.3633718490600586\n",
      "Epoch 1:, Iteration: 26, Loss: 1.2701301574707031\n",
      "Epoch 1:, Iteration: 27, Loss: 1.3438845872879028\n",
      "Epoch 1:, Iteration: 28, Loss: 1.0371365547180176\n",
      "Epoch 1:, Iteration: 29, Loss: 1.1139256954193115\n",
      "Epoch 1:, Iteration: 30, Loss: 1.1494327783584595\n",
      "Epoch 1:, Iteration: 31, Loss: 1.1350141763687134\n",
      "Epoch 1:, Iteration: 32, Loss: 1.0157991647720337\n",
      "Epoch 1:, Iteration: 33, Loss: 1.0754748582839966\n",
      "Epoch 1:, Iteration: 34, Loss: 1.1070345640182495\n",
      "Epoch 1:, Iteration: 35, Loss: 0.9019622802734375\n",
      "Epoch 1:, Iteration: 36, Loss: 0.971652626991272\n",
      "Epoch 1:, Iteration: 37, Loss: 0.9211934804916382\n",
      "Epoch 1:, Iteration: 38, Loss: 1.028660535812378\n",
      "Epoch 1:, Iteration: 39, Loss: 1.0814201831817627\n",
      "Epoch 1:, Iteration: 40, Loss: 0.9148903489112854\n",
      "Epoch 1:, Iteration: 41, Loss: 0.6901594996452332\n",
      "Epoch 1:, Iteration: 42, Loss: 0.9129964709281921\n",
      "Epoch 1:, Iteration: 43, Loss: 0.9678287506103516\n",
      "Epoch 1:, Iteration: 44, Loss: 0.7238381505012512\n",
      "Epoch 1:, Iteration: 45, Loss: 0.902362585067749\n",
      "Epoch 1:, Iteration: 46, Loss: 0.8102775812149048\n",
      "Epoch 1:, Iteration: 47, Loss: 0.9333319664001465\n",
      "Epoch 1:, Iteration: 48, Loss: 0.8642800450325012\n",
      "Epoch 1:, Iteration: 49, Loss: 0.8863214254379272\n",
      "Epoch 1:, Iteration: 50, Loss: 0.8334185481071472\n",
      "Epoch 1:, Iteration: 51, Loss: 0.8035659193992615\n",
      "Epoch 1:, Iteration: 52, Loss: 0.8206921219825745\n",
      "Epoch 1:, Iteration: 53, Loss: 0.8576948046684265\n",
      "Epoch 1:, Iteration: 54, Loss: 0.8229830861091614\n",
      "Epoch 1:, Iteration: 55, Loss: 0.7914471626281738\n",
      "Epoch 1:, Iteration: 56, Loss: 0.8349287509918213\n",
      "Epoch 1:, Iteration: 57, Loss: 0.687238335609436\n",
      "Epoch 1:, Iteration: 58, Loss: 0.7627753019332886\n",
      "Epoch 1:, Iteration: 59, Loss: 0.7542770504951477\n",
      "Epoch 1:, Iteration: 60, Loss: 0.8407499194145203\n",
      "Epoch 1:, Iteration: 61, Loss: 0.6950116157531738\n",
      "Epoch 1:, Iteration: 62, Loss: 0.7810469269752502\n",
      "Epoch 1:, Iteration: 63, Loss: 0.8892679810523987\n",
      "Epoch 1:, Iteration: 64, Loss: 0.7488295435905457\n",
      "Epoch 1:, Iteration: 65, Loss: 0.7777696847915649\n",
      "Epoch 1:, Iteration: 66, Loss: 0.8179137706756592\n",
      "Epoch 1:, Iteration: 67, Loss: 0.6973520517349243\n",
      "Epoch 1:, Iteration: 68, Loss: 0.6652047038078308\n",
      "Epoch 1:, Iteration: 69, Loss: 0.7697582840919495\n",
      "Epoch 1:, Iteration: 70, Loss: 0.8384510278701782\n",
      "Epoch 1:, Iteration: 71, Loss: 0.698459267616272\n",
      "Epoch 1:, Iteration: 72, Loss: 0.722369372844696\n",
      "Epoch 1:, Iteration: 73, Loss: 0.7556918859481812\n",
      "Epoch 1:, Iteration: 74, Loss: 0.7391598224639893\n",
      "Epoch 1:, Iteration: 75, Loss: 0.8113605380058289\n",
      "Epoch 1:, Iteration: 76, Loss: 0.8497410416603088\n",
      "Epoch 1:, Iteration: 77, Loss: 0.7457391619682312\n",
      "Epoch 1:, Iteration: 78, Loss: 0.7842479348182678\n",
      "Epoch 1:, Iteration: 79, Loss: 0.7652149796485901\n",
      "Epoch 1:, Iteration: 80, Loss: 0.7850774526596069\n",
      "Epoch 1:, Iteration: 81, Loss: 0.7073176503181458\n",
      "Epoch 1:, Iteration: 82, Loss: 0.6895044445991516\n",
      "Epoch 1:, Iteration: 83, Loss: 0.6466937065124512\n",
      "Epoch 1:, Iteration: 84, Loss: 0.740149199962616\n",
      "Epoch 1:, Iteration: 85, Loss: 0.7745134234428406\n",
      "Epoch 1:, Iteration: 86, Loss: 0.7437136769294739\n",
      "Epoch 1:, Iteration: 87, Loss: 0.7449347376823425\n",
      "Epoch 1:, Iteration: 88, Loss: 0.8673811554908752\n",
      "Epoch 1:, Iteration: 89, Loss: 0.695686936378479\n",
      "Epoch 1:, Iteration: 90, Loss: 0.7686672806739807\n",
      "Epoch 1:, Iteration: 91, Loss: 0.7531213760375977\n",
      "Epoch 1:, Iteration: 92, Loss: 0.7998415231704712\n",
      "Epoch 1:, Iteration: 93, Loss: 0.8105474710464478\n",
      "Epoch 1:, Iteration: 94, Loss: 0.7832781076431274\n",
      "Epoch 1:, Iteration: 95, Loss: 0.6467830538749695\n",
      "Epoch 1:, Iteration: 96, Loss: 0.7711556553840637\n",
      "Epoch 1:, Iteration: 97, Loss: 0.7710387706756592\n",
      "Epoch 1:, Iteration: 98, Loss: 0.7704617381095886\n",
      "Epoch 1:, Iteration: 99, Loss: 0.6862668991088867\n",
      "Epoch 1:, Iteration: 100, Loss: 0.7353322505950928\n",
      "Epoch 1:, Iteration: 101, Loss: 0.7600210905075073\n",
      "Epoch 1:, Iteration: 102, Loss: 0.6356356143951416\n",
      "Epoch 1:, Iteration: 103, Loss: 0.6923831701278687\n",
      "Epoch 1:, Iteration: 104, Loss: 0.6993394494056702\n",
      "Epoch 1:, Iteration: 105, Loss: 0.7559388875961304\n",
      "Epoch 1:, Iteration: 106, Loss: 0.6205887198448181\n",
      "Epoch 1:, Iteration: 107, Loss: 0.7790241241455078\n",
      "Epoch 1:, Iteration: 108, Loss: 0.6845290064811707\n",
      "Epoch 1:, Iteration: 109, Loss: 0.7445457577705383\n",
      "Epoch 1:, Iteration: 110, Loss: 0.6719181537628174\n",
      "Epoch 1:, Iteration: 111, Loss: 0.7414533495903015\n",
      "Epoch 1:, Iteration: 112, Loss: 0.7422586679458618\n",
      "Epoch 1:, Iteration: 113, Loss: 0.577661395072937\n",
      "Epoch 1:, Iteration: 114, Loss: 0.6570109128952026\n",
      "Epoch 1:, Iteration: 115, Loss: 0.7468264102935791\n",
      "Epoch 1:, Iteration: 116, Loss: 0.6651227474212646\n",
      "Epoch 1:, Iteration: 117, Loss: 0.6640828847885132\n",
      "Epoch 1:, Iteration: 118, Loss: 0.6855233311653137\n",
      "Epoch 2:, Iteration: 1, Loss: 0.6253985166549683\n",
      "Epoch 2:, Iteration: 2, Loss: 0.7990838289260864\n",
      "Epoch 2:, Iteration: 3, Loss: 0.6513298749923706\n",
      "Epoch 2:, Iteration: 4, Loss: 0.7031343579292297\n",
      "Epoch 2:, Iteration: 5, Loss: 0.7244082689285278\n",
      "Epoch 2:, Iteration: 6, Loss: 0.6595273613929749\n",
      "Epoch 2:, Iteration: 7, Loss: 0.6071208715438843\n",
      "Epoch 2:, Iteration: 8, Loss: 0.6719530820846558\n",
      "Epoch 2:, Iteration: 9, Loss: 0.6759824156761169\n",
      "Epoch 2:, Iteration: 10, Loss: 0.677645742893219\n",
      "Epoch 2:, Iteration: 11, Loss: 0.6988245248794556\n",
      "Epoch 2:, Iteration: 12, Loss: 0.660942792892456\n",
      "Epoch 2:, Iteration: 13, Loss: 0.7160204648971558\n",
      "Epoch 2:, Iteration: 14, Loss: 0.6309527158737183\n",
      "Epoch 2:, Iteration: 15, Loss: 0.6728837490081787\n",
      "Epoch 2:, Iteration: 16, Loss: 0.610386312007904\n",
      "Epoch 2:, Iteration: 17, Loss: 0.6900946497917175\n",
      "Epoch 2:, Iteration: 18, Loss: 0.6580064296722412\n",
      "Epoch 2:, Iteration: 19, Loss: 0.6163219213485718\n",
      "Epoch 2:, Iteration: 20, Loss: 0.632823646068573\n",
      "Epoch 2:, Iteration: 21, Loss: 0.6698779463768005\n",
      "Epoch 2:, Iteration: 22, Loss: 0.7260158658027649\n",
      "Epoch 2:, Iteration: 23, Loss: 0.6181433796882629\n",
      "Epoch 2:, Iteration: 24, Loss: 0.6599962711334229\n",
      "Epoch 2:, Iteration: 25, Loss: 0.6114168167114258\n",
      "Epoch 2:, Iteration: 26, Loss: 0.5486131310462952\n",
      "Epoch 2:, Iteration: 27, Loss: 0.6316306591033936\n",
      "Epoch 2:, Iteration: 28, Loss: 0.5304973721504211\n",
      "Epoch 2:, Iteration: 29, Loss: 0.6459163427352905\n",
      "Epoch 2:, Iteration: 30, Loss: 0.6144480109214783\n",
      "Epoch 2:, Iteration: 31, Loss: 0.6207566261291504\n",
      "Epoch 2:, Iteration: 32, Loss: 0.6798073053359985\n",
      "Epoch 2:, Iteration: 33, Loss: 0.6348137855529785\n",
      "Epoch 2:, Iteration: 34, Loss: 0.7381179928779602\n",
      "Epoch 2:, Iteration: 35, Loss: 0.5922617316246033\n",
      "Epoch 2:, Iteration: 36, Loss: 0.6147840023040771\n",
      "Epoch 2:, Iteration: 37, Loss: 0.6088050603866577\n",
      "Epoch 2:, Iteration: 38, Loss: 0.5933470129966736\n",
      "Epoch 2:, Iteration: 39, Loss: 0.6829255819320679\n",
      "Epoch 2:, Iteration: 40, Loss: 0.5340784788131714\n",
      "Epoch 2:, Iteration: 41, Loss: 0.6768924593925476\n",
      "Epoch 2:, Iteration: 42, Loss: 0.5850476622581482\n",
      "Epoch 2:, Iteration: 43, Loss: 0.6075876355171204\n",
      "Epoch 2:, Iteration: 44, Loss: 0.5855720639228821\n",
      "Epoch 2:, Iteration: 45, Loss: 0.6660269498825073\n",
      "Epoch 2:, Iteration: 46, Loss: 0.5737093687057495\n",
      "Epoch 2:, Iteration: 47, Loss: 0.6350339651107788\n",
      "Epoch 2:, Iteration: 48, Loss: 0.5830262899398804\n",
      "Epoch 2:, Iteration: 49, Loss: 0.5988842844963074\n",
      "Epoch 2:, Iteration: 50, Loss: 0.581365704536438\n",
      "Epoch 2:, Iteration: 51, Loss: 0.6472733616828918\n",
      "Epoch 2:, Iteration: 52, Loss: 0.6839070320129395\n",
      "Epoch 2:, Iteration: 53, Loss: 0.6069484949111938\n",
      "Epoch 2:, Iteration: 54, Loss: 0.5648338794708252\n",
      "Epoch 2:, Iteration: 55, Loss: 0.6400084495544434\n",
      "Epoch 2:, Iteration: 56, Loss: 0.6900782585144043\n",
      "Epoch 2:, Iteration: 57, Loss: 0.626059889793396\n",
      "Epoch 2:, Iteration: 58, Loss: 0.5922776460647583\n",
      "Epoch 2:, Iteration: 59, Loss: 0.5797073245048523\n",
      "Epoch 2:, Iteration: 60, Loss: 0.5248329639434814\n",
      "Epoch 2:, Iteration: 61, Loss: 0.5516371726989746\n",
      "Epoch 2:, Iteration: 62, Loss: 0.6264119744300842\n",
      "Epoch 2:, Iteration: 63, Loss: 0.5736518502235413\n",
      "Epoch 2:, Iteration: 64, Loss: 0.6398847103118896\n",
      "Epoch 2:, Iteration: 65, Loss: 0.6213411688804626\n",
      "Epoch 2:, Iteration: 66, Loss: 0.5820671319961548\n",
      "Epoch 2:, Iteration: 67, Loss: 0.5874521732330322\n",
      "Epoch 2:, Iteration: 68, Loss: 0.5792248845100403\n",
      "Epoch 2:, Iteration: 69, Loss: 0.6118732690811157\n",
      "Epoch 2:, Iteration: 70, Loss: 0.6705186367034912\n",
      "Epoch 2:, Iteration: 71, Loss: 0.6876631379127502\n",
      "Epoch 2:, Iteration: 72, Loss: 0.6383452415466309\n",
      "Epoch 2:, Iteration: 73, Loss: 0.6168370246887207\n",
      "Epoch 2:, Iteration: 74, Loss: 0.6720825433731079\n",
      "Epoch 2:, Iteration: 75, Loss: 0.5634220838546753\n",
      "Epoch 2:, Iteration: 76, Loss: 0.6230372786521912\n",
      "Epoch 2:, Iteration: 77, Loss: 0.539011538028717\n",
      "Epoch 2:, Iteration: 78, Loss: 0.5426921844482422\n",
      "Epoch 2:, Iteration: 79, Loss: 0.5781247019767761\n",
      "Epoch 2:, Iteration: 80, Loss: 0.6116552948951721\n",
      "Epoch 2:, Iteration: 81, Loss: 0.5488376617431641\n",
      "Epoch 2:, Iteration: 82, Loss: 0.5413876175880432\n",
      "Epoch 2:, Iteration: 83, Loss: 0.6212248802185059\n",
      "Epoch 2:, Iteration: 84, Loss: 0.6719254851341248\n",
      "Epoch 2:, Iteration: 85, Loss: 0.5637654066085815\n",
      "Epoch 2:, Iteration: 86, Loss: 0.6597496271133423\n",
      "Epoch 2:, Iteration: 87, Loss: 0.5746135115623474\n",
      "Epoch 2:, Iteration: 88, Loss: 0.6222459077835083\n",
      "Epoch 2:, Iteration: 89, Loss: 0.5276364088058472\n",
      "Epoch 2:, Iteration: 90, Loss: 0.5560315847396851\n",
      "Epoch 2:, Iteration: 91, Loss: 0.5972070693969727\n",
      "Epoch 2:, Iteration: 92, Loss: 0.5797758102416992\n",
      "Epoch 2:, Iteration: 93, Loss: 0.5475108623504639\n",
      "Epoch 2:, Iteration: 94, Loss: 0.6380163431167603\n",
      "Epoch 2:, Iteration: 95, Loss: 0.5350554585456848\n",
      "Epoch 2:, Iteration: 96, Loss: 0.6437616348266602\n",
      "Epoch 2:, Iteration: 97, Loss: 0.5366783738136292\n",
      "Epoch 2:, Iteration: 98, Loss: 0.5857399106025696\n",
      "Epoch 2:, Iteration: 99, Loss: 0.5196927785873413\n",
      "Epoch 2:, Iteration: 100, Loss: 0.6015799641609192\n",
      "Epoch 2:, Iteration: 101, Loss: 0.6020366549491882\n",
      "Epoch 2:, Iteration: 102, Loss: 0.6102673411369324\n",
      "Epoch 2:, Iteration: 103, Loss: 0.6166198253631592\n",
      "Epoch 2:, Iteration: 104, Loss: 0.5845724940299988\n",
      "Epoch 2:, Iteration: 105, Loss: 0.5279527902603149\n",
      "Epoch 2:, Iteration: 106, Loss: 0.5445982813835144\n",
      "Epoch 2:, Iteration: 107, Loss: 0.6950119733810425\n",
      "Epoch 2:, Iteration: 108, Loss: 0.5671816468238831\n",
      "Epoch 2:, Iteration: 109, Loss: 0.6147988438606262\n",
      "Epoch 2:, Iteration: 110, Loss: 0.5967903137207031\n",
      "Epoch 2:, Iteration: 111, Loss: 0.4982955753803253\n",
      "Epoch 2:, Iteration: 112, Loss: 0.6170096397399902\n",
      "Epoch 2:, Iteration: 113, Loss: 0.5962809920310974\n",
      "Epoch 2:, Iteration: 114, Loss: 0.5377570390701294\n",
      "Epoch 2:, Iteration: 115, Loss: 0.5290377736091614\n",
      "Epoch 2:, Iteration: 116, Loss: 0.5760557651519775\n",
      "Epoch 2:, Iteration: 117, Loss: 0.5284032821655273\n",
      "Epoch 2:, Iteration: 118, Loss: 0.696090042591095\n",
      "Learning Done!\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 28 * 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "vanilla_fnn_object = VanillaFFN(INPUT_SIZE, NUM_CLASSES)\n",
    "vanilla_fnn_object.to(device=device)\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vanilla_fnn_object.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(2):\n",
    "    loss = 0\n",
    "    for iteration, (data, targets) in enumerate(train_data_loader):\n",
    "        # assign data and targets to device.\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # forward pass, compute losses, backpropagate.\n",
    "        outputs = vanilla_fnn_object(data)\n",
    "        loss = loss_criterion(outputs, targets)\n",
    "        print(\"Epoch {}:, Iteration: {}, Loss: {}\".format(epoch + 1, iteration + 1, loss))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(\"Learning Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up testing data and data loader\n",
    "test_data = datasets.MNIST(root='./data',\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor()\n",
    "                                  ]))\n",
    "\n",
    "test_data_loader = DataLoader(dataset=test_data,\n",
    "                              shuffle=True,\n",
    "                              batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7828, device='cuda:0') 10000\n",
      "Testing Accuracy of Vanilla FNN model on the FMNIST dataset: 78.28\n"
     ]
    }
   ],
   "source": [
    "# Lets check the accuracy on the testing set.\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "vanilla_fnn_object.eval()    # to set in eval mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, (data, targets) in enumerate(test_data_loader):\n",
    "        # assign data and targets to device.\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        outputs = vanilla_fnn_object(data)\n",
    "        \n",
    "        values, index_of_max_value = outputs.max(1)\n",
    "        num_correct += (index_of_max_value == targets).sum()\n",
    "        num_samples += targets.size(0)\n",
    "    \n",
    "    print(num_correct, num_samples)\n",
    "    print(\"Testing Accuracy of Vanilla FNN model on the FMNIST dataset: {}\".format((num_correct.item() / num_samples) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
