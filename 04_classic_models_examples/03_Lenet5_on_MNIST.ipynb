{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LeNet-5 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Class for LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us set up the Lenet Class\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=6, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
    "        self.max_pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1 ,1), padding=(0, 0))\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.max_pool(torch.tanh(self.conv1(x))))\n",
    "        x = torch.tanh(self.max_pool(torch.tanh(self.conv2(x))))\n",
    "        x = torch.tanh(self.conv3(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Sanity check for LeNet-5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "# lets do a sanity test on LeNet object\n",
    "lenet5_test = Lenet5(1, 10)\n",
    "random_input_batch = torch.randn(512, 1, 32, 32)\n",
    "print(lenet5_test(random_input_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing mean and std for our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1000), tensor(0.2752))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us set up the training dataset\n",
    "mean_std_data = datasets.MNIST(root='./data',\n",
    "                                   download=True,\n",
    "                                   train=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Pad(padding=2, fill=0, padding_mode='constant'),\n",
    "                                       transforms.ToTensor(),\n",
    "                                   ]))\n",
    "\n",
    "# lets compute the mean and std to normalize our training dataset later on.\n",
    "# (MNIST is small enough to do mean, std computation this way)\n",
    "mean_std_loader = DataLoader(dataset=mean_std_data,\n",
    "                             batch_size=len(mean_std_data))\n",
    "data = next(iter(mean_std_loader))\n",
    "train_mean = data[0].mean()\n",
    "train_std = data[0].std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data with transforms used in LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Since LeNet requires 32 x 32 images as per the paper, \n",
    "    the MNIST images were obviously padded (since their dimensions are 28 x 28).\n",
    "    The reason that LeCun cites in the paper is that its better for the stroke-ends \n",
    "    and corners to be centered in the receptive field of the high level feature detectors\n",
    "    (probably didn't go for a bigger field size because the largest character in the mnist \n",
    "    dataset has a size of 20 x 20).\n",
    "\"\"\"\n",
    "\n",
    "# let us set up the training dataset\n",
    "train_data = datasets.MNIST(root='./data',\n",
    "                                   download=True,\n",
    "                                   train=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Pad(padding=2, fill=0, padding_mode='constant'),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((train_mean,), (train_std,))\n",
    "                                   ]))\n",
    "# and now the dataloader\n",
    "train_data_loader = DataLoader(dataset=train_data,\n",
    "                               shuffle=True,\n",
    "                               batch_size=512,\n",
    "                               num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Sanity Check for train data loader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# sanity check: Check dimensions of a single batch of data\n",
    "for idx, (data, target) in enumerate(train_data_loader):\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 1.584311604499817\n",
      "Epoch: 2, loss: 1.4870234727859497\n",
      "Epoch: 3, loss: 1.511109471321106\n",
      "Epoch: 4, loss: 1.5001393556594849\n",
      "Epoch: 5, loss: 1.4633334875106812\n",
      "Epoch: 6, loss: 1.4766794443130493\n",
      "Epoch: 7, loss: 1.4706932306289673\n",
      "Epoch: 8, loss: 1.4775718450546265\n",
      "Epoch: 9, loss: 1.4733105897903442\n",
      "Epoch: 10, loss: 1.4676822423934937\n",
      "Epoch: 11, loss: 1.4747141599655151\n",
      "Epoch: 12, loss: 1.4788789749145508\n",
      "Epoch: 13, loss: 1.463736653327942\n",
      "Epoch: 14, loss: 1.4723554849624634\n",
      "Epoch: 15, loss: 1.4745911359786987\n",
      "Epoch: 16, loss: 1.4618431329727173\n",
      "Epoch: 17, loss: 1.4613240957260132\n",
      "Epoch: 18, loss: 1.4646424055099487\n",
      "Epoch: 19, loss: 1.4699339866638184\n",
      "Epoch: 20, loss: 1.4721182584762573\n",
      "Done Learning!\n"
     ]
    }
   ],
   "source": [
    "# set up device, hyperparameters and the training loop\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CHANNELS=1\n",
    "NUM_CLASSES=10\n",
    "\n",
    "lenet5 = Lenet5(NUM_CHANNELS, NUM_CLASSES).to(device=device)\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lenet5.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for idx, (data, target) in enumerate(train_data_loader):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "        outputs = lenet5(data)\n",
    "\n",
    "        loss = loss_criterion(outputs, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: {}, loss: {}\".format(epoch + 1, loss))\n",
    "print(\"Done Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Mean, std for test data (and loading in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1015), tensor(0.2774))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us set up the testing dataset for mean, std computation\n",
    "mean_std_data_test = datasets.MNIST(root='./data',\n",
    "                                   download=True,\n",
    "                                   train=False,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Pad(padding=2, fill=0, padding_mode='constant'),\n",
    "                                       transforms.ToTensor(),\n",
    "                                   ]))\n",
    "\n",
    "# lets compute the mean and std to normalize our testing dataset later on \n",
    "# (MNIST is small enough to do mean, std computation this way)\n",
    "mean_std_loader_test = DataLoader(dataset=mean_std_data_test,\n",
    "                             batch_size=len(mean_std_data_test))\n",
    "data = next(iter(mean_std_loader_test))\n",
    "test_mean = data[0].mean()\n",
    "test_std = data[0].std()\n",
    "test_mean, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up test data and test data loader\n",
    "test_data = datasets.MNIST(root='./data', \n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.Pad(padding=2, fill=0, padding_mode='constant'),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((test_mean,), (test_std,))\n",
    "                                  ]))\n",
    "\n",
    "test_data_loader = DataLoader(dataset=test_data,\n",
    "                              shuffle=True,\n",
    "                              batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9888, device='cuda:0') 10000\n",
      "Accuracy of model on the test set: 98.88\n"
     ]
    }
   ],
   "source": [
    "#98.28 with tanh after maxpool (no normalization)\n",
    "#98.08 without tanh after maxpool (no normalization)\n",
    "#98.88 with tanh after maxpool (normalized)\n",
    "#98.59 without tanh after maxpool(normalized)\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "\n",
    "lenet5.eval()\n",
    "# disable gradient computation for test set\n",
    "with torch.no_grad():\n",
    "    for idx, (data, targets) in enumerate(test_data_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        outputs = lenet5(data)\n",
    "        values, idx_of_max_value = outputs.max(1)\n",
    "        num_correct += (idx_of_max_value == targets).sum()\n",
    "        num_samples += targets.shape[0]\n",
    "    print(num_correct, num_samples)\n",
    "    print(\"Accuracy of model on the test set: {}\".format((num_correct.item() / num_samples) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
