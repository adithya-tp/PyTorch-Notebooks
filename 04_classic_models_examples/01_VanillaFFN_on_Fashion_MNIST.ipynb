{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the class for Vanilla FFN\n",
    "class VanillaFFN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(VanillaFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=input_size // 2)\n",
    "        self.fc2 = nn.Linear(in_features=input_size // 2, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# model sanity_check\n",
    "sample_net = VanillaFFN(784, 10)\n",
    "print(sample_net(torch.randn(64, 784)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d80b4f175045a288ae4219acb38e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926802086b1240488ecd55327059a1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a9ac34c3b14ee798b7a286ad6eac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e19bab32764d5595862298ac4a30a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset\n",
    "train_data = datasets.FashionMNIST(root='./data', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor()\n",
    "                                   ]),\n",
    "                                   download=True)\n",
    "# Set up the data loader for our training data.\n",
    "train_data_loader = DataLoader(dataset=train_data, \n",
    "                               shuffle=True,\n",
    "                               batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([512, 1, 28, 28]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# sanity check for train data loader.\n",
    "for idx, data_tuple in enumerate(train_data_loader):\n",
    "    print(data_tuple[0].shape, data_tuple[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and setup device tensor computations should be assigned to\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:, Iteration: 1, Loss: 2.3008344173431396\n",
      "Epoch 1:, Iteration: 2, Loss: 40.24032211303711\n",
      "Epoch 1:, Iteration: 3, Loss: 199.291259765625\n",
      "Epoch 1:, Iteration: 4, Loss: 253.36212158203125\n",
      "Epoch 1:, Iteration: 5, Loss: 148.43064880371094\n",
      "Epoch 1:, Iteration: 6, Loss: 73.64132690429688\n",
      "Epoch 1:, Iteration: 7, Loss: 21.558216094970703\n",
      "Epoch 1:, Iteration: 8, Loss: 15.872228622436523\n",
      "Epoch 1:, Iteration: 9, Loss: 7.039508819580078\n",
      "Epoch 1:, Iteration: 10, Loss: 3.765284776687622\n",
      "Epoch 1:, Iteration: 11, Loss: 3.8672595024108887\n",
      "Epoch 1:, Iteration: 12, Loss: 3.136260747909546\n",
      "Epoch 1:, Iteration: 13, Loss: 2.366891860961914\n",
      "Epoch 1:, Iteration: 14, Loss: 2.2154788970947266\n",
      "Epoch 1:, Iteration: 15, Loss: 2.349421262741089\n",
      "Epoch 1:, Iteration: 16, Loss: 2.056389570236206\n",
      "Epoch 1:, Iteration: 17, Loss: 1.727394938468933\n",
      "Epoch 1:, Iteration: 18, Loss: 1.765850305557251\n",
      "Epoch 1:, Iteration: 19, Loss: 1.9894050359725952\n",
      "Epoch 1:, Iteration: 20, Loss: 1.6393438577651978\n",
      "Epoch 1:, Iteration: 21, Loss: 1.6180897951126099\n",
      "Epoch 1:, Iteration: 22, Loss: 1.760765552520752\n",
      "Epoch 1:, Iteration: 23, Loss: 1.6961452960968018\n",
      "Epoch 1:, Iteration: 24, Loss: 1.521151065826416\n",
      "Epoch 1:, Iteration: 25, Loss: 1.667643666267395\n",
      "Epoch 1:, Iteration: 26, Loss: 1.6412482261657715\n",
      "Epoch 1:, Iteration: 27, Loss: 1.3534191846847534\n",
      "Epoch 1:, Iteration: 28, Loss: 1.3053995370864868\n",
      "Epoch 1:, Iteration: 29, Loss: 1.3439533710479736\n",
      "Epoch 1:, Iteration: 30, Loss: 1.6110587120056152\n",
      "Epoch 1:, Iteration: 31, Loss: 1.3187925815582275\n",
      "Epoch 1:, Iteration: 32, Loss: 1.1953802108764648\n",
      "Epoch 1:, Iteration: 33, Loss: 1.3889962434768677\n",
      "Epoch 1:, Iteration: 34, Loss: 1.373618483543396\n",
      "Epoch 1:, Iteration: 35, Loss: 1.2650258541107178\n",
      "Epoch 1:, Iteration: 36, Loss: 1.102466344833374\n",
      "Epoch 1:, Iteration: 37, Loss: 1.2620407342910767\n",
      "Epoch 1:, Iteration: 38, Loss: 1.277677297592163\n",
      "Epoch 1:, Iteration: 39, Loss: 1.2645947933197021\n",
      "Epoch 1:, Iteration: 40, Loss: 1.1791619062423706\n",
      "Epoch 1:, Iteration: 41, Loss: 1.157531976699829\n",
      "Epoch 1:, Iteration: 42, Loss: 1.0581799745559692\n",
      "Epoch 1:, Iteration: 43, Loss: 1.1158517599105835\n",
      "Epoch 1:, Iteration: 44, Loss: 1.193365216255188\n",
      "Epoch 1:, Iteration: 45, Loss: 1.163033366203308\n",
      "Epoch 1:, Iteration: 46, Loss: 1.1093500852584839\n",
      "Epoch 1:, Iteration: 47, Loss: 1.1227670907974243\n",
      "Epoch 1:, Iteration: 48, Loss: 1.0856646299362183\n",
      "Epoch 1:, Iteration: 49, Loss: 1.0794597864151\n",
      "Epoch 1:, Iteration: 50, Loss: 1.002941370010376\n",
      "Epoch 1:, Iteration: 51, Loss: 1.0525553226470947\n",
      "Epoch 1:, Iteration: 52, Loss: 1.1214258670806885\n",
      "Epoch 1:, Iteration: 53, Loss: 1.0992263555526733\n",
      "Epoch 1:, Iteration: 54, Loss: 1.0933213233947754\n",
      "Epoch 1:, Iteration: 55, Loss: 1.0802613496780396\n",
      "Epoch 1:, Iteration: 56, Loss: 0.9936245679855347\n",
      "Epoch 1:, Iteration: 57, Loss: 1.1207159757614136\n",
      "Epoch 1:, Iteration: 58, Loss: 0.9994446039199829\n",
      "Epoch 1:, Iteration: 59, Loss: 0.955260157585144\n",
      "Epoch 1:, Iteration: 60, Loss: 0.9145736694335938\n",
      "Epoch 1:, Iteration: 61, Loss: 0.9845144748687744\n",
      "Epoch 1:, Iteration: 62, Loss: 1.0132977962493896\n",
      "Epoch 1:, Iteration: 63, Loss: 0.9761828184127808\n",
      "Epoch 1:, Iteration: 64, Loss: 0.8614236116409302\n",
      "Epoch 1:, Iteration: 65, Loss: 0.9341490864753723\n",
      "Epoch 1:, Iteration: 66, Loss: 0.9150336980819702\n",
      "Epoch 1:, Iteration: 67, Loss: 0.9933109879493713\n",
      "Epoch 1:, Iteration: 68, Loss: 0.8217343091964722\n",
      "Epoch 1:, Iteration: 69, Loss: 0.8545663356781006\n",
      "Epoch 1:, Iteration: 70, Loss: 0.8719146847724915\n",
      "Epoch 1:, Iteration: 71, Loss: 1.0091018676757812\n",
      "Epoch 1:, Iteration: 72, Loss: 0.8448743224143982\n",
      "Epoch 1:, Iteration: 73, Loss: 0.961290180683136\n",
      "Epoch 1:, Iteration: 74, Loss: 0.9261288642883301\n",
      "Epoch 1:, Iteration: 75, Loss: 0.8192776441574097\n",
      "Epoch 1:, Iteration: 76, Loss: 0.9735797643661499\n",
      "Epoch 1:, Iteration: 77, Loss: 0.8349132537841797\n",
      "Epoch 1:, Iteration: 78, Loss: 0.8072019815444946\n",
      "Epoch 1:, Iteration: 79, Loss: 1.0032979249954224\n",
      "Epoch 1:, Iteration: 80, Loss: 0.9454935193061829\n",
      "Epoch 1:, Iteration: 81, Loss: 0.9469653367996216\n",
      "Epoch 1:, Iteration: 82, Loss: 0.968627393245697\n",
      "Epoch 1:, Iteration: 83, Loss: 0.8738534450531006\n",
      "Epoch 1:, Iteration: 84, Loss: 0.9604595303535461\n",
      "Epoch 1:, Iteration: 85, Loss: 0.9293584823608398\n",
      "Epoch 1:, Iteration: 86, Loss: 0.9730873703956604\n",
      "Epoch 1:, Iteration: 87, Loss: 0.8597016930580139\n",
      "Epoch 1:, Iteration: 88, Loss: 0.9031713604927063\n",
      "Epoch 1:, Iteration: 89, Loss: 0.9769717454910278\n",
      "Epoch 1:, Iteration: 90, Loss: 0.9151110649108887\n",
      "Epoch 1:, Iteration: 91, Loss: 0.9659938812255859\n",
      "Epoch 1:, Iteration: 92, Loss: 0.8579897880554199\n",
      "Epoch 1:, Iteration: 93, Loss: 0.8324304223060608\n",
      "Epoch 1:, Iteration: 94, Loss: 0.9515265226364136\n",
      "Epoch 1:, Iteration: 95, Loss: 0.8407352566719055\n",
      "Epoch 1:, Iteration: 96, Loss: 0.919653058052063\n",
      "Epoch 1:, Iteration: 97, Loss: 0.7767115831375122\n",
      "Epoch 1:, Iteration: 98, Loss: 0.9156322479248047\n",
      "Epoch 1:, Iteration: 99, Loss: 0.8743384480476379\n",
      "Epoch 1:, Iteration: 100, Loss: 0.9724354147911072\n",
      "Epoch 1:, Iteration: 101, Loss: 0.781782329082489\n",
      "Epoch 1:, Iteration: 102, Loss: 0.9512991309165955\n",
      "Epoch 1:, Iteration: 103, Loss: 0.8636082410812378\n",
      "Epoch 1:, Iteration: 104, Loss: 0.8555592894554138\n",
      "Epoch 1:, Iteration: 105, Loss: 0.9014414548873901\n",
      "Epoch 1:, Iteration: 106, Loss: 0.8880595564842224\n",
      "Epoch 1:, Iteration: 107, Loss: 0.901993989944458\n",
      "Epoch 1:, Iteration: 108, Loss: 0.8434221148490906\n",
      "Epoch 1:, Iteration: 109, Loss: 0.8529238104820251\n",
      "Epoch 1:, Iteration: 110, Loss: 1.0306521654129028\n",
      "Epoch 1:, Iteration: 111, Loss: 0.9839706420898438\n",
      "Epoch 1:, Iteration: 112, Loss: 0.914456844329834\n",
      "Epoch 1:, Iteration: 113, Loss: 0.9936843514442444\n",
      "Epoch 1:, Iteration: 114, Loss: 0.8963940739631653\n",
      "Epoch 1:, Iteration: 115, Loss: 0.7969249486923218\n",
      "Epoch 1:, Iteration: 116, Loss: 0.9248769283294678\n",
      "Epoch 1:, Iteration: 117, Loss: 0.8979725241661072\n",
      "Epoch 1:, Iteration: 118, Loss: 0.9278321862220764\n",
      "Epoch 2:, Iteration: 1, Loss: 0.9147419929504395\n",
      "Epoch 2:, Iteration: 2, Loss: 0.8692473769187927\n",
      "Epoch 2:, Iteration: 3, Loss: 0.9059723615646362\n",
      "Epoch 2:, Iteration: 4, Loss: 0.8422133922576904\n",
      "Epoch 2:, Iteration: 5, Loss: 0.8415791988372803\n",
      "Epoch 2:, Iteration: 6, Loss: 0.9381573796272278\n",
      "Epoch 2:, Iteration: 7, Loss: 0.8971768021583557\n",
      "Epoch 2:, Iteration: 8, Loss: 0.8892837762832642\n",
      "Epoch 2:, Iteration: 9, Loss: 0.8611949682235718\n",
      "Epoch 2:, Iteration: 10, Loss: 0.8791306614875793\n",
      "Epoch 2:, Iteration: 11, Loss: 0.9426416158676147\n",
      "Epoch 2:, Iteration: 12, Loss: 0.8722032904624939\n",
      "Epoch 2:, Iteration: 13, Loss: 0.8347228765487671\n",
      "Epoch 2:, Iteration: 14, Loss: 0.8004025220870972\n",
      "Epoch 2:, Iteration: 15, Loss: 0.8066811561584473\n",
      "Epoch 2:, Iteration: 16, Loss: 0.8220772743225098\n",
      "Epoch 2:, Iteration: 17, Loss: 0.7897087931632996\n",
      "Epoch 2:, Iteration: 18, Loss: 0.9548042416572571\n",
      "Epoch 2:, Iteration: 19, Loss: 0.7914150357246399\n",
      "Epoch 2:, Iteration: 20, Loss: 0.8449311852455139\n",
      "Epoch 2:, Iteration: 21, Loss: 0.8778162002563477\n",
      "Epoch 2:, Iteration: 22, Loss: 0.9406031966209412\n",
      "Epoch 2:, Iteration: 23, Loss: 0.7192168235778809\n",
      "Epoch 2:, Iteration: 24, Loss: 0.7613627910614014\n",
      "Epoch 2:, Iteration: 25, Loss: 0.9037309885025024\n",
      "Epoch 2:, Iteration: 26, Loss: 0.7267225384712219\n",
      "Epoch 2:, Iteration: 27, Loss: 0.762390673160553\n",
      "Epoch 2:, Iteration: 28, Loss: 0.7421084642410278\n",
      "Epoch 2:, Iteration: 29, Loss: 0.7969065308570862\n",
      "Epoch 2:, Iteration: 30, Loss: 0.7241747975349426\n",
      "Epoch 2:, Iteration: 31, Loss: 0.8045204281806946\n",
      "Epoch 2:, Iteration: 32, Loss: 0.6775869131088257\n",
      "Epoch 2:, Iteration: 33, Loss: 0.7974831461906433\n",
      "Epoch 2:, Iteration: 34, Loss: 0.8067123293876648\n",
      "Epoch 2:, Iteration: 35, Loss: 0.8902583122253418\n",
      "Epoch 2:, Iteration: 36, Loss: 0.7087585926055908\n",
      "Epoch 2:, Iteration: 37, Loss: 0.7437083721160889\n",
      "Epoch 2:, Iteration: 38, Loss: 0.7303543090820312\n",
      "Epoch 2:, Iteration: 39, Loss: 0.7632443904876709\n",
      "Epoch 2:, Iteration: 40, Loss: 0.8176873326301575\n",
      "Epoch 2:, Iteration: 41, Loss: 0.8874868750572205\n",
      "Epoch 2:, Iteration: 42, Loss: 0.8089346289634705\n",
      "Epoch 2:, Iteration: 43, Loss: 0.8292336463928223\n",
      "Epoch 2:, Iteration: 44, Loss: 0.7786164879798889\n",
      "Epoch 2:, Iteration: 45, Loss: 0.6945527195930481\n",
      "Epoch 2:, Iteration: 46, Loss: 0.6632590889930725\n",
      "Epoch 2:, Iteration: 47, Loss: 0.6710870862007141\n",
      "Epoch 2:, Iteration: 48, Loss: 0.7868555188179016\n",
      "Epoch 2:, Iteration: 49, Loss: 0.6990889310836792\n",
      "Epoch 2:, Iteration: 50, Loss: 0.7373212575912476\n",
      "Epoch 2:, Iteration: 51, Loss: 0.6939364671707153\n",
      "Epoch 2:, Iteration: 52, Loss: 0.7658467888832092\n",
      "Epoch 2:, Iteration: 53, Loss: 0.7674235105514526\n",
      "Epoch 2:, Iteration: 54, Loss: 0.7608550190925598\n",
      "Epoch 2:, Iteration: 55, Loss: 0.684749186038971\n",
      "Epoch 2:, Iteration: 56, Loss: 0.772258996963501\n",
      "Epoch 2:, Iteration: 57, Loss: 0.6930819153785706\n",
      "Epoch 2:, Iteration: 58, Loss: 0.799850344657898\n",
      "Epoch 2:, Iteration: 59, Loss: 0.8751351237297058\n",
      "Epoch 2:, Iteration: 60, Loss: 0.7382346987724304\n",
      "Epoch 2:, Iteration: 61, Loss: 0.7705575227737427\n",
      "Epoch 2:, Iteration: 62, Loss: 0.7281128168106079\n",
      "Epoch 2:, Iteration: 63, Loss: 0.7909653782844543\n",
      "Epoch 2:, Iteration: 64, Loss: 0.7863602638244629\n",
      "Epoch 2:, Iteration: 65, Loss: 0.6602332592010498\n",
      "Epoch 2:, Iteration: 66, Loss: 0.721641480922699\n",
      "Epoch 2:, Iteration: 67, Loss: 0.6938014626502991\n",
      "Epoch 2:, Iteration: 68, Loss: 0.680215060710907\n",
      "Epoch 2:, Iteration: 69, Loss: 0.7425960898399353\n",
      "Epoch 2:, Iteration: 70, Loss: 0.6656419634819031\n",
      "Epoch 2:, Iteration: 71, Loss: 0.7619166970252991\n",
      "Epoch 2:, Iteration: 72, Loss: 0.6445916891098022\n",
      "Epoch 2:, Iteration: 73, Loss: 0.7487248778343201\n",
      "Epoch 2:, Iteration: 74, Loss: 0.8485087752342224\n",
      "Epoch 2:, Iteration: 75, Loss: 0.6512969136238098\n",
      "Epoch 2:, Iteration: 76, Loss: 0.6784911751747131\n",
      "Epoch 2:, Iteration: 77, Loss: 0.6909001469612122\n",
      "Epoch 2:, Iteration: 78, Loss: 0.7332647442817688\n",
      "Epoch 2:, Iteration: 79, Loss: 0.7587944865226746\n",
      "Epoch 2:, Iteration: 80, Loss: 0.8116123080253601\n",
      "Epoch 2:, Iteration: 81, Loss: 0.7315313220024109\n",
      "Epoch 2:, Iteration: 82, Loss: 0.7225446701049805\n",
      "Epoch 2:, Iteration: 83, Loss: 0.7745895981788635\n",
      "Epoch 2:, Iteration: 84, Loss: 0.838777482509613\n",
      "Epoch 2:, Iteration: 85, Loss: 0.7420957684516907\n",
      "Epoch 2:, Iteration: 86, Loss: 0.7084492444992065\n",
      "Epoch 2:, Iteration: 87, Loss: 0.7032778263092041\n",
      "Epoch 2:, Iteration: 88, Loss: 0.7379938960075378\n",
      "Epoch 2:, Iteration: 89, Loss: 0.6904968619346619\n",
      "Epoch 2:, Iteration: 90, Loss: 0.703364908695221\n",
      "Epoch 2:, Iteration: 91, Loss: 0.7188279628753662\n",
      "Epoch 2:, Iteration: 92, Loss: 0.7442635297775269\n",
      "Epoch 2:, Iteration: 93, Loss: 0.755138099193573\n",
      "Epoch 2:, Iteration: 94, Loss: 0.6425797939300537\n",
      "Epoch 2:, Iteration: 95, Loss: 0.603515088558197\n",
      "Epoch 2:, Iteration: 96, Loss: 0.5939836502075195\n",
      "Epoch 2:, Iteration: 97, Loss: 0.6470031142234802\n",
      "Epoch 2:, Iteration: 98, Loss: 0.7187508940696716\n",
      "Epoch 2:, Iteration: 99, Loss: 0.583309531211853\n",
      "Epoch 2:, Iteration: 100, Loss: 0.6638743877410889\n",
      "Epoch 2:, Iteration: 101, Loss: 0.7022557258605957\n",
      "Epoch 2:, Iteration: 102, Loss: 0.7780197262763977\n",
      "Epoch 2:, Iteration: 103, Loss: 0.6990160346031189\n",
      "Epoch 2:, Iteration: 104, Loss: 0.7851830720901489\n",
      "Epoch 2:, Iteration: 105, Loss: 0.6631485223770142\n",
      "Epoch 2:, Iteration: 106, Loss: 0.7109718918800354\n",
      "Epoch 2:, Iteration: 107, Loss: 0.6745249629020691\n",
      "Epoch 2:, Iteration: 108, Loss: 0.5808085203170776\n",
      "Epoch 2:, Iteration: 109, Loss: 0.6462186574935913\n",
      "Epoch 2:, Iteration: 110, Loss: 0.691569983959198\n",
      "Epoch 2:, Iteration: 111, Loss: 0.668360710144043\n",
      "Epoch 2:, Iteration: 112, Loss: 0.7883579134941101\n",
      "Epoch 2:, Iteration: 113, Loss: 0.7151722311973572\n",
      "Epoch 2:, Iteration: 114, Loss: 0.7670037746429443\n",
      "Epoch 2:, Iteration: 115, Loss: 0.7486851811408997\n",
      "Epoch 2:, Iteration: 116, Loss: 0.6949052810668945\n",
      "Epoch 2:, Iteration: 117, Loss: 0.5802721381187439\n",
      "Epoch 2:, Iteration: 118, Loss: 0.6399063467979431\n",
      "Learning Done!\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 28 * 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "vanilla_fnn_object = VanillaFFN(INPUT_SIZE, NUM_CLASSES)\n",
    "vanilla_fnn_object.to(device=device)\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vanilla_fnn_object.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(2):\n",
    "    loss = 0\n",
    "    for iteration, (data, targets) in enumerate(train_data_loader):\n",
    "        # assign data and targets to device.\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # forward pass, compute losses, backpropagate.\n",
    "        outputs = vanilla_fnn_object(data)\n",
    "        loss = loss_criterion(outputs, targets)\n",
    "        print(\"Epoch {}:, Iteration: {}, Loss: {}\".format(epoch + 1, iteration + 1, loss))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(\"Learning Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up testing data and data loader\n",
    "test_data = datasets.FashionMNIST(root='./data',\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor()\n",
    "                                  ]))\n",
    "\n",
    "test_data_loader = DataLoader(dataset=test_data,\n",
    "                              shuffle=True,\n",
    "                              batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7144, device='cuda:0') 10000\n",
      "Testing Accuracy of Vanilla FNN model on the FMNIST dataset: 71.44\n"
     ]
    }
   ],
   "source": [
    "# Lets check the accuracy on the testing set.\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "vanilla_fnn_object.eval()    # to set in eval mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, (data, targets) in enumerate(test_data_loader):\n",
    "        # assign data and targets to device.\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        outputs = vanilla_fnn_object(data)\n",
    "        \n",
    "        values, index_of_max_value = outputs.max(1)\n",
    "        num_correct += (index_of_max_value == targets).sum()\n",
    "        num_samples += targets.size(0)\n",
    "    \n",
    "    print(num_correct, num_samples)\n",
    "    print(\"Testing Accuracy of Vanilla FNN model on the FMNIST dataset: {}\".format((num_correct.item() / num_samples) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
