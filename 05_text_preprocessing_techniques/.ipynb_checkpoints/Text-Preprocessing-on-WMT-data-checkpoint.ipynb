{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Overview of TorchText's abilities\n",
    "\n",
    "- **File Loading**: load in the corpus from various formats.\n",
    "\n",
    "- **Tokenization**: break sentences into list of words.\n",
    "\n",
    "- **Numericalize/Indexify**: Map words into integer numbers for the entire corpus\n",
    "\n",
    "- **Word Vector**: either initialize vocabulary randomly or load in from a pretrained embedding. This embedding must be \"trimmed\", meaning we only store words in our vocabulary into memory.\n",
    "\n",
    "- **Batching**: Generate batches of training sample (padding is normally happening here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff that it can't do\n",
    "- **Train/Val/Test split**: seperate your data into fixed train/val/test set.\n",
    "- **Embedding Lookup**: Map each sentence (which contain word indices) to fixed dimension word. vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field helps us to specify how the preprocessing should be done.\n",
    "# TabularDataset helps us load data from JSON/CSV/TSV files.\n",
    "# BucketIterator helps us batch and pad the data.\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we're looking to accomplish:\n",
    "- **Tokenization** - Break up a sentence into tokens.\n",
    "- **Build Vocab** -Build a one to one mapping of each unique word (vocab) in the sentence / dataset to an index.\n",
    "- **Numericalize text through vocab lookup** - Substitute each word in the sentence / index with \n",
    "- **Emedding lookup** - Replace each index in a sequence with a semantically meaningful embedding instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data, split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_txt = open('WMT-English-German-Data/train.en', encoding='utf8').read().split('\\n')\n",
    "german_txt = open('WMT-English-German-Data/train.de', encoding='utf8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting by newlines alone causes some mismatch at around the 2000 line mark in the dataset.\n",
    "# Let us restrict our selves to the first 1000 pairs of data instead (these we know align exactly).\n",
    "raw_data = {'English': [line for line in english_txt[:1000]],\n",
    "            'German': [line for line in german_txt[:1000]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe so that we may pass this to sklearn for train_test_split\n",
    "translation_data = pd.DataFrame(raw_data, columns=['English', 'German'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(translation_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files as json formats, so that we can load these files into TabularDataset later.\n",
    "train.to_json('WMT-English-German-Data/translation-train.json', orient='records', lines=True)\n",
    "test.to_json('WMT-English-German-Data/translation-test.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load in our spacy tokenizers\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "\n",
    "# lets define the tokenizing routines using the above loaded tokenizers.\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store field objects for the source and target languages. \n",
    "# These fields will hold the vocabulary for the respective languages,\n",
    "# as well directives that pass in, on how to tokenize / numericalize\n",
    "# the text data (here we use spacy).\n",
    "english = Field(sequential=True, use_vocab=True, tokenize=tokenize_en, lower=True, batch_first=True)\n",
    "german = Field(sequential=True, use_vocab=True, tokenize=tokenize_de, lower=True, batch_first=True)\n",
    "\n",
    "fields = {'English': ('en', english), 'German': ('de', german)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now construct our numerical datasets based on the fields we just created.\n",
    "train_data, test_data = TabularDataset.splits(path='',\n",
    "                                              train='WMT-English-German-Data/translation-train.json',\n",
    "                                              test='WMT-English-German-Data/translation-test.json',\n",
    "                                              format='json',\n",
    "                                              fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though we set use_vocab=True, we can't build a vocab without loading in the data.\n",
    "# Now that we've linked the columns of the train and test data with their respective fields,\n",
    "# we are now in position where we can ask the field to build up a vocab\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the datasets are cleaned up, we can set up a iterator, \n",
    "# that'll serve us batches of data.\n",
    "train_iterator, test_iterator = BucketIterator.splits((train_data, test_data),\n",
    "                                                      batch_size=32,\n",
    "                                                      device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14, 378,   3,  ...,   1,   1,   1],\n",
      "        [420,  82,   3,  ...,   1,   1,   1],\n",
      "        [286,   6, 163,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [443,  13, 109,  ...,   1,   1,   1],\n",
      "        [367,   6,  87,  ...,   4,   1,   1],\n",
      "        [160, 602, 409,  ...,   1,   1,   1]], device='cuda:0')\n",
      "torch.Size([32, 53])\n",
      "tensor([[1538,  256,  178,  ...,    1,    1,    1],\n",
      "        [ 398,   83,   54,  ...,    1,    1,    1],\n",
      "        [ 286,    5,  553,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [ 111,  370, 1286,  ...,    1,    1,    1],\n",
      "        [ 252,    5,   61,  ...,    1,    1,    1],\n",
      "        [ 149,  386,    0,  ...,    1,    1,    1]], device='cuda:0')\n",
      "torch.Size([32, 56])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch.en)\n",
    "    print(batch.en.shape)\n",
    "    print(batch.de)\n",
    "    print(batch.de.shape)\n",
    "    # You see that most of the sequences end with '1's.\n",
    "    # This is the number that the \"pad token\" got mapped to.\n",
    "    # The iterator object took care of that automatically for each batch that we iterate over.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
